Faster Convergence and More Accurate Models: How
Anomaly Detection and Exclusion Enhances Federated
Learning

Anonymous Author(s)
Affiliation
Address
email

Abstract

This paper investigates how principled anomaly detection in Federated Learning
(FL) can make the learning more efficient and effective, while also addressing
the traditional trade-off between system robustness and performance. We prove
theoretically and verify on practical cases that detecting and removing anomalous
models, whether due to adversarial behavior or data corruption, benefits learning
efficiency by boosting convergence and producing more accurate models. Specifically, we conduct a theoretical investigation of FL convergence with and without
defenses that detect anomalous models and exclude corresponding clients, which
proves that removing clients supplying anomalous updates in conventional FL
algorithms results in faster convergence. While state-of-the-art anomaly detection
mechanisms typically introduce an additional quadratic computational overhead,
we reduce the anomaly detection computational complexity by introducing a novel
Proportional-Integral–Derivative-inspired Model Anomaly Detection and Exclusion (PID-MADE) algorithm for not only detecting anomalous clients but also
excluding them from the training process. This approach complements standard
aggregation strategies, such as FedAvg, preserving the original linear time complexity. Empirical evaluation on several benchmark datasets confirms that our
method, combined with standard FL aggregation, not only improves security by
effectively identifying and removing anomalous clients but also enhances learning
efficiency compared to state-of-the-art approaches. The results emphasize that
anomaly detection measures in FL, which improve security and privacy protection,
can coexist with, and even enhance learning efficiency, providing a more effective
framework for federated model training.

Introduction

Federated Learning (FL) is a decentralized Machine Learning (ML) paradigm that enables multiple
clients to collaboratively train a shared model without exposing their private data (1). FL is becoming
indispensable in modern ML privacy-sensitive applications in various domains, such as healthcare and
finance, where data sharing is restricted due to ethical and legal constraints. However, FL systems are
prone to learning efficiency degradation due to anomalous client updates, which can occur because of
malicious actions or data corruption.(2; 3). Introducing anomaly detection and exclusion mechanisms
imposes computational and communication overhead, potentially slowing down the model’s convergence. For developers, balancing the trade-off between exclusion effectiveness, learning efficiency
and fairness is particularly challenging when transitioning to production environments, where both
system’s security and integrity and learning efficiency are critical.

Submitted to 39th Conference on Neural Information Processing Systems (NeurIPS 2025). Do not distribute.

Algorithm
PID-MADE (Ours)
Krum
Multi-Krum
Bulyan
RFA

Computation
O(nd)O(nd)O(nd)
O(n2d)
O(n2d)
O(n2d)
O(n2d)

#MC Estimation? Temporal Adaptivity?

No
Yes
Yes
Yes
No

Yes
No
No
No
No

Table 1: Comparison of our PID-inspired approach with previous work in terms of the computational burden and
requirement on the prior knowledge of the number of malicious clients (#MC) in the system

Several defense mechanisms have been proposed to detect and exclude anomalous client updates,
typically by analyzing gradients or model updates using distance-based metrics, but these methods
often incur significant overhead, risk excluding benign clients, and rely on prior estimates of the
number of anomalies, which are rarely accurate in practice (4; 5; 6).

To address these challenges, we propose a novel approach inspired by the Proportional-IntegralDerivative (PID) control concept, widely used in engineering, to enhance the security and performance
of FL systems. As the approach is targeted at Model Anomaly Detection and Exclusion (MADE), we
name it PID-MADE. PID-MADE introduces a scoring mechanism that incorporates both current and
historical client behavior, using a distance metric to compute PID-based scores for client updates.
Clients whose scores exceed a threshold are flagged as anomalous and excluded from the aggregation
process, which results in reducing computational costs, training time, and improving the learning
effectiveness and efficiency of the FL system.

In this paper, we present the following contributions. (1) We provide a formal theoretical analysis
demonstrating that any FL algorithm incorporating MADE provably converges. (2) We further
provide theoretical and empirical evidence demonstrating that FL with MADE converges faster than
undefended FL in the presence of anomalies. (3) We introduce PID-MADE, a novel FL anomalies
detection mechanism designed to reduce computational burdens while incorporating temporal information about clients. A key advantage of PID-MADE is that it does not require the estimated number
of malicious clients, unlike methods such as Krum and its derivatives. We provide a formal analysis
demonstrating PID-MADE’s linear computational complexity and provide theoretically-grounded
recommendations on setting anomaly detection thresholds for FL. Experimental results demonstrate
that PID-MADE performs on par with, and in some cases outperforms, state-of-the-art defenses in
terms of learning efficiency while growing linear in time. (4) To facilitate broader adoption and
further research, we implement our approach as a software framework, which we made available
to the public1 to benefit the academic and professional communities. Together, these contributions
demonstrate that enhancing FL with MADE systems can simultaneously improve learning efficiency
and effectiveness, providing an improved framework for federated model training.

2 Related Work

Anomalies in FL generally can fall into two categories: malicious, such as (7; 8; 9; 10; 11; 12; 13; 14;
15), and technological occurring due to inherent noise in data. A variety of defense and anomaly detection mechanisms have been introduced. Similarity-based defenses, such as (4), focus on identifying
robust aggregation strategies to reduce the influence of outlier updates. Privacy-preserving methods
such as differential privacy (DP) (16; 17) and homomorphic encryption (18) provide guarantees for
protecting client data but introduce computational and communication overhead. Secure aggregation
and secure multi-party computation (SMPC) (19; 20) enable encrypted communication and ensure the
integrity of the aggregated updates. ELSA (21), RoFL (22), or Prio (23) aim to improve FL security
holistically by integrating secure aggregation mechanisms, cryptographic techniques, protocol-level
modifications, and using multiple-server architectures. While effective, these approaches typically
require substantial changes to the FL workflow and incur significant computational costs, thus not
considered in this work.

1https://drive.google.com/file/d/1VSTeE6ynMPQcnGUu_nIZO0_mkQdni8DH/view?usp=drive_link

(anonymized)

Symbol
i
t
wi
t

µt

di
t

A
G ⊂ A

wA
t

wG
t

w∗

µG
t
µA
t

Description
Client index
Communication round
Set of weights sent by client i to the server at round
t
Model parameters centroid computed by the server
at round t
Euclidean distance of the i-th client model from the
centroid at round t
Set of all clients participating in the learning process
Subset of “good” clients not excluded during learning
Set of weights for all clients in A at round t: A =
i|A|
i2
t , . . . , w
t

{w
Set of weights for all clients in G at round t: G =
i|G|
i2
t , . . . , w
t

{w
Optimal model parameters that minimize the loss
function
Centroid of the “good” models at round t
Centroid of all models at round t

i1
t , w

i1
t , w

}

}

Weights Hyperspace

(cid:13)
A\G
(cid:13)
(cid:13)w
t

− w∗(cid:13)
(cid:13)
(cid:13)

M

µ

Legend:

weights in A \ G
weights in G
optimal weights
centroid µ

w∗

(cid:13)
(cid:13)wG
(cid:13)

t − w∗(cid:13)

(cid:13)
(cid:13)

Figure 2: Visualization of definition (1) in the hyperspace
of model weights. M is the margin that allows us to include
some of the weights that are further away from the majority
of good weights

Figure 1: Notation used in the paper

We compare our method against state-of-the-art FL defenses, such as Krum and Multi-Krum (5),
Bulyan (4), and RFA (6). Krum and Multi-Krum use geometric distances to select updates that are
closest to others in the parameter space, making them robust against malicious clients. However,
they require prior knowledge of the maximum number of malicious clients, which is impractical in
real-world scenarios, and both have a computational complexity of O(n2d), where n is the number
of clients and d is the gradient space’s dimension. Bulyan extends these methods with additional
filtering steps for improved robustness but inherits the same computational complexity. RFA employs
the geometric median instead of the mean to aggregate updates. Unlike Krum-based methods, RFA
does not require an estimation of malicious clients. Computing the geometric median, which involves
iterative algorithms like Weiszfeld’s with a complexity of O(n2d), is computationally intensive,
making RFA less scalable in real-world deployments.

Our PID-MADE approach addresses these limitations by improving the computational complexity
and eliminating the need for prior knowledge of the number of malicious clients. It incorporates
an integral component accounting for the historical behavior of client updates for more reliable
evaluations over time. Unlike previous methods that assess clients in isolation per round, PID-MADE
leverages temporal adaptivity to detect persistent anomalies across training rounds, thereby improving
robustness to the impacts of these anomalies. Furthermore, our approach enhances security while
improving learning efficiency, accelerating model training compared to the baseline methods. We
formally analyze the computational complexity and convergence of our approach in Sec. 3. In Table
1 we compare our approach with Krum, Multi-Krum, Bulyan, and RFA in terms of computational
complexity, and the need for prior knowledge used as an input to the system.

3 FL Defense Formal Analysis

In this section, we present a theoretical analysis to evaluate the feasibility of enhancing FL learning
efficiency by excluding anomalous clients from the aggregation process. Our approach involves
formulating lemmas and theorems, and analyzing the convergence of the FL process with and without
clients sending anomalous updates. First, we establish that model anomaly detection and exclusion
accelerates convergence, and then we quantify the acceleration. Figure 1 summarizes the notation
and terminology used throughout the paper. Due to space limitations we present all the proofs in the
Appendix.

We propose FL defense that aims to identify and separate anomalous clients (“bad”) from benign ones
(“good”) to prevent biased model updates that hinder convergence. This is achieved by analyzing
the distribution of model updates and removing outliers based on their distance from the centroid
position. This can be described by the following definition of anomalous model weights.

Definition 1 (Anomalous Model Weights): We say that weights submitted by a client are anomalous
if they satisfy the following separation condition: Assume some training round t. The minimal

t ) and the optimal model, plus the margin M : minA\G

distance between the aggregated anomalous client model updates (wA\G
) and the optimal model (w∗)
must be greater than the maximum distance between the aggregated “good” client model updates
t − w∗(cid:13)
t − w∗(cid:13)
(wG
(cid:13)
(cid:13)
(cid:13) + M ,
(cid:13) > maxG
where M is a sensitivity margin for outliers. Figure 2 illustrates Definition 1. As M increases, more
outlier clients may join G. The centroid µt is used as a proxy for the optimal model, with malicious
clients (red) further from it than benign ones (blue). The defense works as long as |G| > |A \ G|, i.e.
the honest majority persists.

(cid:13)
(cid:13)wA\G
(cid:13)

(cid:13)
(cid:13)wG
(cid:13)

t

Criterion 1 (Anomaly Signature in FL): In real-world FL deployments, some client updates may
deviate drastically from the benign population because of an attacker’s poisoned data or simply
corrupted measurements. We treat any such persistently “outlying” update as an anomaly. Formally,
t − w∗(cid:13)
we say an anomaly in FL satisfies: ∀ε > 0, ∄N ∈ N s.t. ∀t ≥ N,
(cid:13)
(cid:13) < ε. In other words,
no matter how small a tolerance ε we choose, there is no round after which some client i’s updates
remain within that tolerance of w∗. This criterion follows from Definition 1.

(cid:13)
(cid:13)
(cid:13)wA

3.1 Convergence

Lemma 1 (Variance Reduction through Outlier Removal): Let {ai} be a set of points on a number
line with scalar values where ai ∈ R, i ∈ N and a1 < a2 < . . . < aN , N ≥ 2. We consider one of
those points, aN , an outlier point ao, meaning that ao satisfies (ao − µ)2 = max1≤i≤N (ai − µ)2.
Let us form a new set of points by simply removing ao from the original set. Then, if σ2 is the
variance of the original set and σ′2 is the variance of the new set, we have that σ′2 ≤ σ2.

Theorem 1 (Convergence Preservation under Anomalous Model Exclusion): Consider global modt and mG composed by the aggregation
els mA composed by the aggregation of all local models wA
t − w∗(cid:13)
of models after exclusion wG
(cid:13)
(cid:13) < ε,
t − w∗(cid:13)
(cid:13)
(cid:13) < ε. That is, assuming the original learning algorithm

t through FedAvg. If ∀ε > 0, ∃N1 ∈ N s.t. ∀t ≥ N1,
(cid:13)
(cid:13)wG
(cid:13)

then ∃N2 ∈ N s.t. ∀t ≥ N2,
converges, an algorithm augmented with anomaly detection and exclusion also converges.

(cid:13)
(cid:13)
(cid:13)wA

Commentary: removing anomalous clients from the FL aggregation does not violate the convergence
of the original algorithm if it still converges even under the attacks or anomalies. If the original model
mA does converge, this implies that the attack is not strong enough, which in practice can occur due
to various reasons, such as a low proportion of malicious clients or the attack goal was to make it
converge to the wrong model (10). With the convergence of mA, we can guarantee that if the anomaly
detection and exclusion is applied, mG will always converge to the correct model and faster than mA,
which is shown in the next part of the theorem. Furthermore, we make a stronger assumption that
even if mA does not converge, mG will still converge. While we do not have a theoretical guarantee,
this is suggested by the empirical evidence, which we present in Sec. 5.

Theorem 2 (Accelerated Convergence under Anomaly Exclusion): If N1 is the round, on which the
t −w∗(cid:13)
(cid:13)
conventional FL with all clients (no clients removed) converges on wA
(cid:13) <
ε, and N2 is the round, on which FL with good clients only (some bad clients are removed) converges
on wG

t − w∗(cid:13)
(cid:13)
(cid:13) < ε, then N2 ≤ N1.

t , that is ∀t ≥ N2,

t , that is ∀t ≥ N1,

(cid:13)
(cid:13)
(cid:13)wA

(cid:13)
(cid:13)wG
(cid:13)

Commentary: the implication of this theorem is that, when using only the updates from clients
without outlier updates (as in wG
k ), the convergence towards the optimal model will be faster than
when aggregating updates from all clients, including those with outlier updates (as in wA
k ). This
is because the outlier updates, which may significantly deviate from the optimal model, distort the
global model, causing it to remain far from an optimal solution for a longer period. In contrast, when
outlier updates are excluded, the model converges faster to the optimal model, as the updates are
more consistent and less prone to distortion. The result shows that the anomaly detection mechanism
that excludes outlier updates from the aggregation improves FL convergence, thus accelerating the
learning process in FL. In Sec. 5, we demonstrate our verification of the convergence on practical use
cases. While the preceding theorems establish improved convergence, the next result shows a general
upper-bound on this accelerated rate (Theorem 3).

Theorem 3 (Enhanced Convergence Rate under Anomaly Exclusion): The norm of the distances
between good models’ and optimal model weights is bounded by the distances between all models’

t − w∗(cid:13)
and optimal model weights, that is ∃N ∈ N s.t. ∀t ≥ N, (cid:13)
constant if the number of malicious clients does not change during learning and C =

t − w∗(cid:13)

(cid:13) ≤ C(cid:13)

(cid:13)wA

(cid:13)wG

(cid:13), where C is a
(cid:113) |G|
|A| ≤ 1.

Proof Sketch: by removing clients sending anomalous updates to the server, we remove the outliers
in the weights dimension, which also reduces the variance, as we show in Lemma 1. Comparing
the variance of benign model weights around the centroid µG
t and the variance of all model weights
around µA
t , and further rewriting in vector notation using the Euclidean norm, the bound follows by
taking the square root.

G at
Commentary: the practical significance of this relationship is that the model with weights wt
some round t ≥ N will converge quicker than wA
|G| times closer
to the optimal model than the weights wA
t . Moreover, as long as |G| > |A \ G|, with the number of
anomalous clients |A \ G| growing, the model protected by our defense will converge quicker at a rate
which depends on |A \ G| if the number of anomalous clients increases during learning. Specifically,

t , and the weights wG

t will be

(cid:113) |A|

if we take the derivative of

(cid:113) |G|

proportional to

( |A\G|
gain in the convergence rate diminishes.

|G| +1)3

(cid:113)

|A\G|+|G| with respect to |A\G|
. This also means that as the number of anomalous clients grows, the

, the convergence increases with a rate

|G|

4 PID-MADE Approach

We develop a novel PID control-inspired algorithm to detect and exclude anomalous updates from FL
aggregation. PID provides for a feedback mechanism with three components – proportional, integral,
and derivative – widely used in automated control systems since its formalization by (24). The goal of
the PID controller is to minimize the error value e(t) over time by adjusting the control variable c(t).
The error is calculated as the difference between the setpoint and the control variable. The control
function c(t) is given by c(t) = Kpe(t) + KI
, where e(t) is the error value at
time t, and the coefficients Kp, KI , and Kd determine the weights of the proportional, integral, and
derivative components.

(cid:82) t
0 e(φ)dφ + Kd

de(t)
dt

In our approach, the proportional term reacts to instantaneous deviations, the integral term identifies
persistent drifts by accounting for historical trends, and the derivative term anticipates future changes.
These components together enable the effective detection of an abnormal client behavior. Our
algorithm measures the error as the distance between a client’s updates and the centroid. It aims at
flagging and excluding significant deviations from the centroid. In the following, we describe how
we adopt the PID principle for detecting anomalous clients in FL. The error value is calculated as the
Euclidean distance of client i’s model from the centroid µt of all submitted models: D(i)
, µt) =
t
∥w(i)
and the geometric
median: arg min
t −y∥ depending on the setting, we illustrate our findings in the following

t − µt∥. Although we defined the centroid µt as both the mean 1
N

i=0 w(i)

t (w(i)

i=0 ∥w(i)

(cid:80)N

(cid:80)N

t

y

sections with the mean estimate. The PID score for each client i is calculated as:

t = D(i)
u(i)
(cid:124)

t (w(i)
t
(cid:123)(cid:122)
proportional

, µt)
(cid:125)

t−1
(cid:88)

x=0

+ KI

(cid:124)

D(i)

x (w(i)

x , µx)

(cid:123)(cid:122)
integral

(cid:125)

+ Kd(D(i)

t (w(i)

t

(cid:124)

t−1(w(i)

, µt) − D(i)
(cid:123)(cid:122)
derivative

t−1, µt−1))
(cid:125)

.

(1)

For each training round: (1) the server distributes the global model to the clients, (2) the clients train
the model locally for a number of epochs and send it back to the server, (3) the server computes the
centroid µt and our PID score as detailed in Formula 1 and excludes any clients above the threshold
τ , derivation of which we describe in Sec. 4.1. The full mechanism integrated with FedAvg is
summarized in Algorithm 2.

4.1 PID-MADE: Anomaly Detection and Exclusion Mechanism

We detect and exclude anomalies by calculating PID scores for each client based on the distance
from µt and comparing them against the threshold τ , which is derived from the upper bound of PID
scores for non-anomalous clients. To implement this mechanism, we address two major challenges:

(1) how to estimate the unknown optimal model and (2) how to estimate the threshold τ . Since the
optimal model is unknown, we estimate it with the centroid µt = 1
at each round. This
N
yields a biased estimate in highly non-IID settings, in which case the geometric median can be used
instead for more robust estimation. To derive the threshold τ , we analyze the PID metric in Formula
1 and first provide a permissive upper bound which is free from assumptions, but leads to a high false
negative rate. To improve this bound, we introduce specific assumptions which allow us to provide a
tighter estimate of τ .

i=0 w(i)

(cid:80)N

t

Theorem 4 (Permissive Upper Bound for Benign PID Scores): The permissive upper bound of the
∆max + O( f
, where ∆max is the maximal deviation
PID score for the good client is given by t ·
from the centroid, f is the number of anomalies, N is the number of all clients, and t is the number
of training rounds. This overly permissive bound provides a zero false-positive rate, but may yield a
high false-negative rate. Although impractical as a detection threshold, it serves as a useful baseline
from which we derive tighter, more effective bound estimates.

(cid:17)
N )

(cid:16)

In Theorems 5 and 6, we provide tighter and more practical upper bounds on PID scores of nonanomalous clients in a cross-device case. Before we introduce Theorems 5 and 6, we present Lemma
2 and Assumption 1 which are necessary for us to prove the theorems.

Lemma 2 (Bounded Centroid Shift): The centroid shift is bounded by O
of Theorem 4).
Let ∆t = ∥w(i)
Assumption 1 (Uncorrelated Deviations): The sequence of random variables {∆t}T
∀0 ≤ t ̸= y ≤ T : Cov(∆t, ∆y) = 0.

t − µt∥ be the deviation of client i’s update from the centroid at any round t.

t=0 satisfies

(cid:17)

(cid:16) f
N

(see Appendix, proof

Although Assumption 1 does not strictly hold in realistic federated settings, nonzero covariances
Cov(∆t, ∆y) can only increase the true variance of the PID score – meaning that the threshold we
derive will be more conservative. Empirically, we observe that applying the threshold τ derived
under Assumption 1 sufficiently separates benign from anomalous clients. A fully rigorous threshold
would account for each pairwise covariance term, however, estimating all Cov(∆t, ∆y) online would
impose significant overhead, and in practice the independence-based approximation already provides
a tight, computationally efficient bound, which we derive in Theorems 5 and 6.

Theorem 5 (Chebyshev Threshold): Let us introduce the random variable Ut representing PID
scores. Under Assumption 1 and using Lemma 2, without knowing the distribution of PID scores,
with probability at most α, the PID scores of good clients will be within zσt of the sample average of
PID scores ¯ut, where σt is the standard deviation of PID scores at round t. Formally, Pr[Ut − ¯ut ≥
zσt] ≤ α, where α = 1√
z represents the desired alarm rate (i.e. false positive rate). With a probability
of at least 1 − α the benign clients will be under the threshold τ = ¯ut + zσt. Equivalently, no more
than α-fraction of benign clients exceed τ . The threshold derivation follows Chebyshev’s inequality
(see the Appendix).
Theorem 6 (Gaussian Threshold): If we assume ∆t ∼ N (µ∆, σ2
also Gaussian, Ut ∼ N (¯ut, σ2
a false positive rate of α, where z1−α is the z-score corresponding to desired α.

∆), then the PID scores become
t ). Then, the exact Gaussian threshold τGauss = ¯ut + z1−ασt ensures

Theorems 5 and 6 give us an opportunity to efficiently select the threshold value based on the
detection statistics we want to achieve in practice and to satisfy specification requirements set up
in an application. To transfer this theoretical foundation into practice and filter out anomalous
clients we compute the expected PID score as sample average ¯ut = 1
t greater
N
than ¯ut + ασt is flagged as an anomaly and excluded from aggregation. As we show in Sec. 5,
this empirical threshold estimation is effective even when ∆t are not Gaussian, which is often
the case in practice. The full algorithm is presented in Algorithm 2, where the input is the set of
client models A and desired alarm rate α, and the output is the non-anomalous client set Q and
the aggregated model. Unlike previous methods (5; 4; 6), our PID-based approach is adaptive and
does not require prior knowledge of the number of malicious clients. The integral term accumulates
historical deviations, making persistent attackers identifiable over time. Additionally, our method has
a linear time complexity of O(nd) which we prove in Lemma 3.

i=0 ui. Any u(i)

(cid:80)N

Algorithm 2 FedAvg with PID-MAD
Input: A, set of clients with private local data, alarm rate α
Output: Q, aggregated global model
Input: A, |A| clients with private local data
Output: Q, aggregated global model
Clients Execute

receive global model from the server
for each local epoch do

execute training algorithm (e.g. SGD)

end for
push the local model wi

t to the aggregation server

Server Executes
Q ← A
for each round t = 1, 2, ... do
t from local clients
, ¯ut, σt

compute µt, u
for each client i ∈ Q do

receive wi

(i)
t

(i)
Q ← Q \ {w
t

: u(i) ≤ τ = ¯u(t) + ασt}

end for
Perform aggregation of weights in Q based on FedAvg.
Distribute aggregated global model back to the clients.

end for

Figure 3: Example images from our study.
Top: ITS; Middle: PneumoniaMNIST; Bottom:
FEMNIST.

Lemma 3 (Computational Complexity): Algorithm 2 runs in O(nd) time, where n refers to the
number of clients and d is the dimension of the model parameter space. Proof : the computation of
the centroid µt and of u(i)
t

, ¯ut, σt are linear O(nd), keeping total complexity linear O(nd).

5 Empirical Evaluation

Anomaly Model: based on Criterion 1 of model anomalies, introduced in Sec. 3, we implement and
evaluate untargeted data poisoning attacks, focusing on a practical case likely in real-world scenarios
(10). Importantly, data poisoning serves as a proxy for a broader class of anomalies, capturing
not only malicious behaviors but also inadvertent deviations arising from corrupted, mislabeled, or
non-representative client data. Thus, our evaluation encompasses both adversarial and non-adversarial
sources of model anomalies.

Emprical Study Setup: we evaluate our PID-based approach on three image datasets: Intelligent
Transportation Systems (ITS), FEMNIST (25), and PneumoniaMNIST (26). From FEMNIST, we
employ a numerical labels subset, and we use the entire PneumoniaMNIST. The ITS dataset consists
of traffic sign images, with around 600 stop sign images and over 3,000 traffic sign images from
the Open Images V6 dataset2. We poison data by flipping labels for certain clients. Each client
uses 90% of their data for training and 10% for validation. Example images are shown in Figure 3.
As a classifier, we employ a convolutional model with a sequence of convolutional, max pooling,
and fully-connected layers, followed by dropout and ReLU activation functions. The final layer is
a softmax with cross-entropy loss. As a FL framework, we employ Flower (27). The experiments
run on a single node with varying numbers of clients. Our PID-MADE aggregation algorithm is
implemented using Flower’s API and we are making it available to the public (see footnote on page
2). In our experiments, we use the mean centroid computation for PID-MADE, where µt is the
average of submitted local model updates, and set the alarm rate α = 2. Our ablation study (see the
Appendix) on selecting and fine-tuning the PID-MADE coefficients demonstrated that KI = 0.8 and
Kd = 0.2, selected heuristically, make anomalous clients more distinguishable from the “good” ones.
The experiments were conducted on a system equipped with an AMD Ryzen 5 7600 CPU, 32 GB of
RAM, and an NVIDIA RTX 4060TI GPU with 16 GB of dedicated memory, running the Ubuntu
22.04 OS.

5.1 Experimental Results

Figures 4(a), 4(b), and 4(c) present the loss function performance of various FL defense mechanisms
across three datasets: FEMNIST (100 rounds, 20 clients, 2 with flipped labels acting as anomalies),
PneumoniaMNIST (50 rounds, 8 clients, 2 anomalous), and ITS (50 rounds, 8 clients, 2 anomalous).
On FEMNIST, while all methods, including the undefended FedAvg baseline, show rapid initial
loss reduction, FedAvg plateaus and even slightly increases towards the end of training, indicating

2https://storage.googleapis.com/openimages/web/download.html

s
s
o
L

0.15

0.1

5 · 10−2

s
s
o
L

0.6

0.4

0.2

Krum
RFA
FedAvg

Multi-krum
Bulyan
PID-MADE

0.2

0.1

Communication Round
(a)

PID-MADE

Multi-krum

Similar loss level for
PID-MADE and Multi-Krum

s
m
g
o
l

,
e
m
T

i

10−1

Communication Round

(d)

0.5

Communication Round

Communication Round

(b)

(c)

1,000

s
m

,
e
m
T

i

Communication Round

(e)

Number of Clients

(f)

Figure 4: Empirical evaluation results: loss demonstrated by various aggregation strategies with and without
defense mechanisms over (a) – FEMNIST (b) – PneumoniaMNIST, and (c) – ITS; (d) – comparison of learning
efficiency and convergence demonstrated by PID-MADE and Multi-Krum on FEMNIST; (e) – changes in score
calculation time taken by each defense method in the training process over FEMNIST; (f) – changes in score
calculation with increasing number of clients for tested defense mechanisms over FEMNIST

convergence difficulties. In contrast, all defenses (Krum, Multi-Krum, Bulyan, RFA, and PID-MADE)
achieve consistently lower loss and more stable convergence than FedAvg after the initial phase,
demonstrating improved learning efficiency. On PneumoniaMNIST, a similar trend is observed:
FedAvg plateaus at a higher loss compared to the defenses. Though some initial volatility is present,
particularly with Multi-Krum, the defenses ultimately converge to significantly lower loss values,
again showing improved learning efficiency. Highlighting the real-world complexities of the ITS
dataset, PID-MADE demonstrates learning efficiency competitive to Krum and FedAvg, and better
than other methods. ITS dataset includes a variety of traffic sign images, notably both traffic signs and
stop signs that are visually very similar. Even with some slight deviation in its loss curve, PID-MADE
maintains its robustness. This suggests PID-MADE’s potential as an efficient anomaly detection and
exclusion mechanism even in the cases with complex real-world data.

Figure 4(d) presents a comparison of the convergence behavior of the proposed PID-MADE mechanism against Multi-Krum. The experiment was conducted with 18 benign and 2 anomalous clients.
The plot illustrates the values of the loss function over communication rounds for both methods.
Initially, both PID-MADE and Multi-Krum show a rapid decrease in loss, indicating effective initial
learning. However, a key difference emerges in their subsequent convergence behavior. PID-MADE
achieves a significantly lower loss earlier in the training process compared to Multi-Krum. Specifically, PID-MADE reaches a stable, low loss within approximately 10 communication rounds, whereas
Multi-Krum takes considerably longer to reach a comparable level of performance, requiring more
than 20 rounds. The faster convergence of PID-MADE demonstrates a clear advantage in terms of
learning efficiency. This results in reduced communication overhead and a faster overall training
time, which is especially beneficial in resource-constrained FL settings.

In Figures 4(e) and 4(f) we demonstrate results on time which various defense algorithms require
to assess the updates on the aggregation server. Here we employed FEMNIST data with 100
communication rounds and 20 clients, 2 of which were anomalous. Figure 4(e) illustrates the change
in score calculation time for each defense mechanism across the 100 communication rounds. Our

Method

Avg FP Avg FN ∆FP ∆FN Recall Precision F1-score

MK f=2
MK f=4
MK f=7
MK f=9
PID-MADE

0.62
0.51
0.43
0.39
0.72
Table 2: Comparison of Detection Metrics Across Methods. 4 clients acting as anomalies.

-3.21
-1.37
1.28
3.10
-

2.14
1.98
1.63
1.45
0.03

0.14
1.98
4.63
6.45
3.35

2.11
1.95
1.60
1.42
-

0.47
0.51
0.59
0.64
0.72

0.93
0.51
0.34
0.28
0.56

PID-MADE consistently exhibits the lowest score calculation time throughout all rounds, remaining
significantly lower (approximately 10−1 ms on the log scale, or roughly 0.1 ms in absolute terms)
compared to all other defenses. Figure 4(f) presents the scaling behavior of the absolute score
calculation time with an increasing number of clients. In comparison to all other evaluated defenses,
our PID-MADE demonstrates a linear increase in calculation time as the number of clients grows.
This suggests that all other defenses tested become more computationally expensive as the number of
clients increases, making them less suitable for large-scale FL scenarios.

Table 2 highlights the classic precision–recall trade-off across methods on FEMNIST dataset and
shows that our PID-based detector achieves the best overall balance. The Multi-Krum variants with
small rejection budgets (e.g. f = 2) incur very few false positives (Avg FP=0.14) and yield high
precision (0.93) but miss many anomalies (Avg FN=2.14, Recall=0.47), resulting in a moderate F1 of
0.62. As f increases, Multi-Krum flags more anomalies (Recall rises from 0.51 to 0.64) but at the
cost of dramatically more false alarms (Avg FP up to 6.45) and sharply reduced precision (down to
0.28), dragging F1 scores below 0.50. By contrast, PID-MADE attains near-perfect detection (Avg
FN ≈ 0, Recall = 0.72) with a moderate false-positive rate (Avg FP = 3.35), yielding both higher
precision (0.56) and the highest F1-score (0.72). PID-MADE outperforms all Multi-Krum settings in
achieving a superior trade-off between sensitivity and specificity.

6 Limitations

The choice of PID coefficients is critical and should be informed by application-specific considerations. We offer the following guidelines: if anomalous clients are expected to exhibit consistent
deviations over time, increasing the coefficient of the integral term can help accumulate and amplify these persistent shifts. Conversely, if immediate discrepancies in submitted updates are more
indicative of anomalies, assigning a higher weight to the proportional term enhances sensitivity
to such deviations. We assume that benign updates share roughly similar deviation patterns. In
highly non-IID environments, where legitimate clients’ data distributions vary dramatically, the PID
threshold can misclassify rare-but-valid updates as anomalies. In this work, we only theoretically analyzed the PID metric’s behavior in a cross-device setting, but verified its practicality in the cross-silo
setting. A deeper theoretical analysis with respect to tailoring the threshold to account for distribution
heterogeneity and cross-silo setting is left for future work.

7 Conclusion

We demonstrated that augmenting FL with anomaly detection and exclusion improves learning efficiency. Our theoretical analysis provided a foundation for understanding how FL anomaly exclusion
mechanisms contribute to faster convergence of the global model. We have shown theoretically
and verified empirically that FL with defenses converges faster than conventional FL. As another
key contribution, we introduced PID-MADE, a novel FL detection mechanism offering several key
advantages over existing approaches. Notably, PID-MADE operates without requiring the estimate
of expected anomalies, unlike other methods such as Krum and its derivatives, freeing users from
specifying this potentially difficult-to-determine parameter in practice. Furthermore, PID-MADE’s
theoretical analysis demonstrated, and empirical validation confirmed, linear computational complexity while maintaining similar or even better learning efficiency, a critical factor for scalability
in large-scale FL deployments. Finally, we also provided theoretically justified recommendations
for threshold selection, which were verified empirically and demonstrated PID-MADE’s superior
performance against state-of-the-art methods.

References

[1] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, “Communication-efficient
learning of deep networks from decentralized data,” in Artificial intelligence and statistics,
pp. 1273–1282, PMLR, 2017.

[2] X. Zhang, Y. Kang, K. Chen, L. Fan, and Q. Yang, “Trading off privacy, utility, and efficiency
in federated learning,” ACM Transactions on Intelligent Systems and Technology, vol. 14, no. 6,
pp. 1–32, 2023.

[3] Z. Yan, D. Li, Z. Zhang, and J. He, “Accuracy–security tradeoff with balanced aggregation and
artificial noise for wireless federated learning,” IEEE Internet of Things Journal, vol. 10, no. 20,
pp. 18154–18167, 2023.

[4] E. M. E. Mhamdi, R. Guerraoui, and S. Rouault, “The Hidden Vulnerability of Distributed
Learning in Byzantium,” July 2018. Issue: arXiv:1802.07927 arXiv: 1802.07927 [cs, stat].

[5] P. Blanchard, E. M. El Mhamdi, R. Guerraoui, and J. Stainer, “Machine learning with adversaries:
Byzantine tolerant gradient descent,” Advances in neural information processing systems, vol. 30,
2017.

[6] K. Pillutla, S. M. Kakade, and Z. Harchaoui, “Robust Aggregation for Federated Learning,”
IEEE Transactions on Signal Processing, vol. 70, pp. 1142–1154, 2022. arXiv: 1912.13445 [cs,
stat].

[7] N. Rodríguez-Barroso, D. Jiménez-López, M. V. Luzón, F. Herrera, and E. Martínez-Cámara,
“Survey on federated learning threats: Concepts, taxonomy on attacks and defences, experimental
study and challenges,” Information Fusion, vol. 90, pp. 148–173, 2023.

[8] V. Tolpegin, S. Truex, M. E. Gursoy, and L. Liu, “Data Poisoning Attacks Against Federated

Learning Systems,” Aug. 2020. arXiv:2007.08432 [cs].

[9] H. Wang, K. Sreenivasan, S. Rajput, H. Vishwakarma, S. Agarwal, J.-y. Sohn, K. Lee, and
D. Papailiopoulos, “Attack of the tails: Yes, you really can backdoor federated learning,”
Advances in Neural Information Processing Systems, vol. 33, pp. 16070–16084, 2020.

[10] V. Shejwalkar, A. Houmansadr, P. Kairouz, and D. Ramage, “Back to the Drawing Board: A
Critical Evaluation of Poisoning Attacks on Production Federated Learning,” in 2022 IEEE
Symposium on Security and Privacy (SP), pp. 1354–1371, IEEE, May 2022. Place: San
Francisco, CA, USA.

[11] G. Baruch, M. Baruch, and Y. Goldberg, “A little is enough: Circumventing defenses for
distributed learning,” Advances in Neural Information Processing Systems, vol. 32, 2019.

[12] A. N. Bhagoji, S. Chakraborty, P. Mittal, and S. Calo, “Analyzing federated learning through an
adversarial lens,” in International conference on machine learning, pp. 634–643, PMLR, 2019.

[13] V. Shejwalkar and A. Houmansadr, “Manipulating the Byzantine: Optimizing Model Poisoning
Attacks and Defenses for Federated Learning,” in Proceedings 2021 Network and Distributed
System Security Symposium, (Virtual), Internet Society, 2021.

[14] M. Fang, X. Cao, J. Jia, and N. Z. Gong, “Local Model Poisoning Attacks to Byzantine-Robust

Federated Learning,” Nov. 2021. Issue: arXiv:1911.11815 arXiv: 1911.11815 [cs].

[15] E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov, “How To Backdoor Federated

Learning,” Aug. 2019. Issue: arXiv:1807.00459 arXiv: 1807.00459.

[16] H. B. McMahan, D. Ramage, K. Talwar, and L. Zhang, “Learning differentially private recurrent

language models,” arXiv preprint arXiv:1710.06963, 2017.

[17] Z. Bu, J. Dong, Q. Long, and W. J. Su, “Deep learning with gaussian differential privacy,”

Harvard data science review, vol. 2020, no. 23, 2020.

[18] J. Ma, S.-A. Naas, S. Sigg, and X. Lyu, “Privacy-preserving federated learning based on multikey homomorphic encryption,” International Journal of Intelligent Systems, vol. 37, no. 9,
pp. 5880–5901, 2022.

[19] P. Kairouz, Z. Liu, and T. Steinke, “The distributed discrete gaussian mechanism for federated
learning with secure aggregation,” in International Conference on Machine Learning, pp. 5201–
5212, PMLR, 2021.

[20] Y. Li, T.-H. Chang, and C.-Y. Chi, “Secure federated averaging algorithm with differential
privacy,” in 2020 IEEE 30th international workshop on machine learning for signal processing
(MLSP), pp. 1–6, IEEE, 2020.

[21] M. Rathee, C. Shen, S. Wagh, and R. A. Popa, “Elsa: Secure aggregation for federated learning
with malicious actors,” in 2023 IEEE Symposium on Security and Privacy (SP), pp. 1961–1979,
IEEE, 2023.

[22] L. Burkhalter, H. Lycklama, A. Viand, N. Küchler, and A. Hithnawi, “Rofl: Attestable robustness

for secure federated learning,” arXiv preprint arXiv:2107.03311, vol. 21, 2021.

[23] H. Corrigan-Gibbs and D. Boneh, “Prio: Private, robust, and scalable computation of aggregate
statistics,” in 14th USENIX symposium on networked systems design and implementation (NSDI
17), pp. 259–282, 2017.

[24] N. Minorsky, “Directional stability of automatically steered bodies,” Journal of the American

Society for Naval Engineers, vol. 34, no. 2, pp. 280–309, 1922.

[25] S. Caldas, S. M. K. Duddu, P. Wu, T. Li, J. Koneˇcn`y, H. B. McMahan, V. Smith, and A. Talwalkar, “Leaf: A benchmark for federated settings,” arXiv preprint arXiv:1812.01097, 2018.

[26] J. Yang, R. Shi, D. Wei, Z. Liu, L. Zhao, B. Ke, H. Pfister, and B. Ni, “Medmnist v2-a large-scale
lightweight benchmark for 2d and 3d biomedical image classification,” Scientific Data, vol. 10,
no. 1, p. 41, 2023.

[27] D. J. Beutel, T. Topal, A. Mathur, X. Qiu, J. Fernandez-Marques, Y. Gao, L. Sani, K. H.
Li, T. Parcollet, P. P. B. de Gusmão, et al., “Flower: A friendly federated learning research
framework,” arXiv preprint arXiv:2007.14390, 2020.

Faster Convergence and More Accurate Models: How
Anomaly Detection and Exclusion Enhances Federated
Learning
(Technical Appendices and Supplementary Materials)

Anonymous Author(s)
Affiliation
Address
email

1 Proofs

1.1 Proof of Criterion 1

Assume that ∀ε > 0, ∃N ∈ N s.t. ∀t ≥ N , ∥wA
t − w∗∥ < ε. This means that by Definition 1
minA\G ∥wA\G
− w∗∥ < ε, which can only happen when there are no anomalies. Hence, we have
reached a contradiction with Definition 1, and thus for every ε > 0 there is no such N for which
∥wA

t − w∗∥ < ε is satisfied.

t

1.2 Proof of Lemma 1

We will show that removing outliers reduces the variance for a set of points on a number line with
scalar values. Let {ai} be a set where ai ∈ R, i ∈ N and a1 < a2 < . . . < aN . We consider one
of those points, aN , an outlier point ao, meaning that ao significantly deviates from the rest of the
points. The mean ¯a of {ai} is given as

If we exclude ao, the new mean ¯a′ is

But (1) can be rewritten as

Equivalently,

¯a =

N

N
(cid:88)

i=1

ai.

¯a′ =

N − 1

N −1
(cid:88)

i=1

ai.

¯a =

N

(cid:32) N −1
(cid:88)

i=1

(cid:33)

ai + ao

¯a =

N − 1
N

¯a′ +

ao
N

¯a − ¯a′ =

ao − ¯a′
N

(1)

(2)

(3)

(4)

(5)

Submitted to 39th Conference on Neural Information Processing Systems (NeurIPS 2025). Do not distribute.

Variance σ2 of the set without outlier removal:
N

σ2 =

N
(cid:88)

(ai − ¯a)2

i=1

σ2 =

N

(cid:32) N −1
(cid:88)

(ai − ¯a)2 + (ao − ¯a)2

(cid:33)

i=1

Variance (σ′)2 of the set with ao removed:

(σ′)2 =

N − 1

N −1
(cid:88)

(ai − ¯a′)2

i=1

The deviation of each term ai around the mean ¯a is

Using (5):

ai − ¯a = ai − ¯a′ − (¯a − ¯a′)

ai − ¯a = ai − ¯a′ −

ao − ¯a′
N

(cid:32)

(ai − ¯a)2 =

ai − ¯a′ −

(cid:33)2

ao − ¯a′
N

= (ai − ¯a′)2 − 2(ai − ¯a′)

(cid:32)

ao − ¯a′
N

(cid:33)

(cid:32)

+

ao − ¯a′
N

(cid:33)2

N −1
(cid:88)

(ai − ¯a)2 =

i=1

N −1
(cid:88)

(ai − ¯a′)2 − 2

i=1

(N − 1)

(cid:33)2

(cid:32)

ao − ¯a′
N

(cid:32)

ao − ¯a′
N

(cid:33) N −1
(cid:88)

i=1

(ai − ¯a′)+

(6)

(7)

(8)

(9)

(10)

(11)

(12)

(13)

(cid:80)N −1

i=1 (ai − ¯a′) = 0 due to sum of deviations around the mean being zero. Then (13) reduces to

N −1
(cid:88)

(ai − ¯a)2 =

i=1

N −1
(cid:88)

(ai − ¯a′)2 + (N − 1)

i=1

(cid:33)2

(cid:32)

ao − ¯a′
N

Plugging (14) into (7) we get

Using (8):

σ2 =

N

(cid:34) N −1
(cid:88)

(ai − ¯a′)2 + (N − 1)

i=1

(cid:33)2

(cid:32)

ao − ¯a′
N

(cid:35)

+ (ao − ¯a)2

σ2 =

N − 1
N

σ′2 +

(N − 1)
N

(cid:33)2

(cid:32)

ao − ¯a′
N

+

(ao − ¯a)2
N

Given that ao is sufficiently large, from (16) it follows that σ2 > σ′2.

(14)

(15)

(16)

1.3 Proof of Theorems 1 and 3

According to lemma 1 (the inequality here is not strict because we might not remove any model
weights at all):

(cid:16)

|G|

(cid:88)

i∈G

wi

t − µG
t

(cid:17)2

≤

(cid:16)

|A|

(cid:88)

j∈A

(cid:17)2

wj

t − µA
t

Multiplying by |G| both sides and additionally multiplying the right side by |A|

|A| yields:

(cid:88)

(cid:16)

wi

t − µG
t

(cid:17)2

≤

i∈G

|G|
|A|

(cid:88)

(cid:16)

(cid:17)2

wj

t − µA
t

j∈A

In vector notation using the Euclidean norm:
(cid:13)
(cid:13)
(cid:13)

t − µG
t

≤

|G|
|A|

(cid:13)
(cid:13)wA
(cid:13)

t − µA
t

(cid:13)
(cid:13)
(cid:13)

Because centroid µA

t minimizes

(cid:13)
(cid:13)wG
(cid:13)
(cid:13)
(cid:13)
(cid:13)wA
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)wG
(cid:13)

t − µG
t

:

(cid:13)
(cid:13)
t − µA
(cid:13)
t
(cid:13)
|G|
(cid:13)wA
(cid:13)
|A|
(cid:13)
(cid:13)µG
(cid:13)

lim
t→∞

≤

≤

(cid:13)
t − µA
(cid:13)
(cid:13)
t
t − w∗(cid:13)
(cid:13)
(cid:13) = 0

|G|
|A|

(cid:13)
(cid:13)wA
(cid:13)

t − µG
t

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)wG
(cid:13)

t − w∗(cid:13)
(cid:13)
(cid:13)

≤

|G|
|A|

(cid:13)
(cid:13)wA
(cid:13)

t − w∗(cid:13)
(cid:13)
(cid:13)

t − w∗(cid:13)
(cid:13)
(cid:13) ≤
In the Appendix, we also provide additional commentary to theorems 1 and 3.

t − w∗(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)wA
(cid:13)

(cid:13)
(cid:13)wG
(cid:13)

|G|
|A|

(cid:115)

For t ≥ N :

Finally,

(17)

(18)

(19)

(20)

(21)

(22)

1.4 Proof of Theorem 2

meaning that in round k

Let assume that possibly N2 > N1 : ∃N1 < k < N2, such that for wA
such that:
k − w∗(cid:13)
(cid:13)
(cid:13) < ε and
k − w∗(cid:13)
(cid:13)
(cid:13) ≤
that contradicts to the definition given in (1). In (24), k is sufficiently large such that the outlier
k , causing it to deviate beyond ε-distance from w∗.
updates significantly affect the global model wA

(cid:13)
(cid:13)wG
(cid:13)
k − w∗(cid:13)
(cid:13)
(cid:13)

k − w∗(cid:13)
(cid:13)
(cid:13) > ε,

k , there exists round k

(cid:13)
(cid:13)wA
(cid:13)

(cid:13)
(cid:13)wA
(cid:13)

(cid:13)
(cid:13)wG
(cid:13)

k and wG

(23)

(24)

Additional commentary to Theorems 1 and 3

into “good” wG
If we further split wA
t
derive the following, more detailed bound for the relation ∥wA
∥wG
|B|
|B| + |G|

t and “bad” wB

t clients (B = {wi1
t −w∗∥
t −w∗∥

t =

wB
t

wA

:

|G|
|B| + |G|
(cid:13)
(cid:13)wG
(cid:13)

wG
t +
t − w∗(cid:13)
(cid:13)
(cid:13) +

|G|
|B| + |G|

(cid:13)
(cid:13)wA
(cid:13)

t − w∗(cid:13)
(cid:13)
(cid:13) =

|B|
|B| + |G|

(cid:13)
(cid:13)wB
(cid:13)

t − w∗(cid:13)
(cid:13)
(cid:13)

t , wi2

t , . . . , w

i|B|
t

}), we can

Dividing both sides by ∥wG
(cid:13)
(cid:13)wA
(cid:13)
(cid:13)wG

t − w∗∥ yields
t − w∗(cid:13)
(cid:13)
t − w∗(cid:13)
(cid:13)

|G|
|B| + |G|
t −w∗∥
In comparison to Theorem 1.2, here we provide an equality, i.e. we can quantify the relation ∥wA
.
∥wG
t −w∗∥
However, since w∗ in practice is unknown our approximation can only be based on µt. This would
further increase the term ∥wB
∥wG

|B|
|B| + |G|

to ∥wB
∥wG

t − w∗(cid:13)
(cid:13)
t − w∗(cid:13)
(cid:13)

(cid:13)
(cid:13)wB
(cid:13)
(cid:13)wG

=

+

.

t −w∗∥
t −w∗∥

t −µt∥
t −µt∥

1.5 Proof of Theorem 4

Proof: PID score:

t = D(i)
u(i)

t (w(i)

t

, µt) + KI

t−1
(cid:88)

x=0

where

Substitute into (25):

D(i)

x (w(i)

x , µx) + Kd(D(i)

t (w(i)

t

D(i)

t (w(i)

t

, µt) =

(cid:13)
(cid:13)w(i)
(cid:13)

t − µt

(cid:13)
(cid:13)
(cid:13).

, µt) − D(i)

t−1(w(i)

t−1, µt−1))

(25)

u(i)
t =

(cid:13)
(cid:13)w(i)
(cid:13)

t − µt

(cid:13)
(cid:13)
(cid:13) + KI

t−1
(cid:88)

x=0

(cid:13)
(cid:13)w(i)
(cid:13)

x − µx

(cid:13)
(cid:13)
(cid:13) + (1 − KI )

(cid:110)(cid:13)
(cid:13)w(i)
(cid:13)

t − µt

(cid:13)
(cid:13)
(cid:13) −

(cid:13)
(cid:13)w(i)
(cid:13)

t−1 − µt−1

(cid:111)

(cid:13)
(cid:13)
(cid:13)

(26)

Assume minority of clients are anomalous, i.e. f < N
first show that the centroid µt = 1
N

i w(i)

(cid:80)

t will not be shifted significantly.

2 , where f is the number of anomalies. Let’s

µt =

N

(cid:88)

w(i)

t =

i

(cid:32)

N

(cid:88)

i∈G

w(i)

t +

(cid:88)

j∈B

(cid:33)
.

w(j)
t

Consider the purely good centroid µG
t :
|G|

t =

µG

w(i)

t =

(cid:88)

i∈G

N − f

(cid:88)

i∈G

w(i)
t

.

Let’s subtract µG

t from both sides of (4):

µt − µG

t =

N

(cid:88)

i∈G

w(i)

t −

N − f

(cid:88)

i∈G

w(i)

t +

N

(cid:88)

j∈B

w(j)
t

.

µt − µG

t =

(cid:32)

N

−

N − f

(cid:33)

(cid:88)

i∈G

w(i)

t +

N

(cid:88)

j∈B

w(j)
t

.

Take the Euclidean norm on both sides and apply triangle inequality:

(cid:13)
(cid:13)
µt − µG
(cid:13)
(cid:13)
t
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

N

−

N − f

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

i∈G

(cid:13)
(cid:13)w(i)
(cid:13)

t

(cid:13)
(cid:13)
(cid:13) +

N

(cid:88)

j∈B

(cid:13)
(cid:13)w(j)
(cid:13)

t

(cid:13)
(cid:13)
(cid:13).

Note that

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

N

−

N − f

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

N − f − N
N (N − f )

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

f
N (N − f )

.

(27)

(28)

(29)

(30)

(31)

Now, assume both anomalous and benign norms are bound with some constant ζ. This means that
(cid:13)
(cid:80)
(cid:13)
(cid:13) ≤ f ζ. Considering this and (31), we rewrite (30) as:

(cid:13)
(cid:13)w(i)
(cid:13)

(cid:13)
(cid:13)w(i)
(cid:13)

j∈B

i∈G

t

t

(cid:13)
(cid:13) ≤ (N − f )ζ and (cid:80)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
µt − µG
(cid:13)
(cid:13)
(cid:13)
(cid:13)
t
(cid:13)
(cid:13)

≤

f (N − f )
N (N − f )

ζ +

f
N

ζ ≤

2f
N

ζ ≤ O

(cid:17)

.

(cid:16) f
N

(32)

(32) Provides an upper bound on the centroid shift, which refer to as Bounded Centroid Shift in
Lemma 2 of the main paper. This lemma allows us to bound the PID score for benign clients. Let’s
analyze each term in (8) by rewriting it with the benign centroid µG
t :

u(i)∈G
t

=

t

(cid:13)
(cid:13)w(i)∈G
(cid:13)
(cid:124)
(cid:123)(cid:122)
P roportional

− µt

+KI

(cid:13)
(cid:13)
(cid:13)
(cid:125)

t−1
(cid:88)

x=0
(cid:124)

− µx

x

(cid:13)
(cid:13)w(i)∈G
(cid:13)
(cid:123)(cid:122)
Integral

+(1−KI )

(cid:110)(cid:13)
(cid:13)w(i)∈G
(cid:13)

t

(cid:124)

(cid:13)
(cid:13)
(cid:13)

(cid:125)

− µt

t−1 − µt−1

(cid:13)
(cid:13)
(cid:13)w(i)∈G
(cid:13)
(cid:13)
(cid:13) −
(cid:123)(cid:122)
Derivative

(cid:111)

(cid:13)
(cid:13)
(cid:13)

.

(cid:125)

(33)

First, consider the proportional part. Add and subtract µG
t :
(cid:13)
(cid:13)
(cid:13)µt − µG
(cid:13)
(cid:13)
(cid:13) ≤

(cid:13)
(cid:13)w(i)∈G
(cid:13)

(cid:13)
(cid:13)w(i)∈G
(cid:13)

− µt + µG

t − µG
t

(cid:13)
(cid:13)
(cid:13) +

− µG
t

t

t

t

(cid:13)
(cid:13)
(cid:13) ≤

(cid:13)
(cid:13)w(i)∈G
(cid:13)

t

− µG
t

(cid:13)
(cid:13)
(cid:13) + O

Assuming bounded heterogeneity between good clients, the upper bound on
∆max. Then (34) is bounded above by ∆max + O( f
Second, look at the integral part. Same manipulation:

N ).

(cid:13)
(cid:13)w(i)∈G
(cid:13)

t

.

(cid:17)

(34)

(cid:16) f
N
(cid:13)
(cid:13)
(cid:13) is some

− µG
t

t−1
(cid:88)

x=0

(cid:13)
(cid:13)w(i)∈G
(cid:13)

x

−µx+µG

t −µG
t

(cid:13)
(cid:13)
(cid:13) ≤

t−1
(cid:88)

x=0

(cid:13)
(cid:13)w(i)∈G
(cid:13)

x

−µG
t

(cid:13)
(cid:13)
(cid:13)+

t−1
(cid:88)

x=0

(cid:13)
(cid:13)µx−µG
(cid:13)

t

(cid:13)
(cid:13)
(cid:13) =

t−1
(cid:88)

x=0

(cid:13)
(cid:13)w(i)∈G
(cid:13)

x

−µG
t

(cid:13)
(cid:13)
(cid:13)+t·O

(cid:16)

= t

∆max+O(

(cid:17)
)

f
N

(cid:17)

(cid:16) f
N
(35)

Third, we do the same analysis on the derivative part:
(cid:13)
(cid:13)w(i)∈G
(cid:13)

(cid:13)
(cid:13)w(i)∈G
(cid:13)

(cid:13)
(cid:13)w(i)∈G
(cid:13)

t−1 −µt−1

(cid:13)
(cid:13)
(cid:13) ≤

(cid:13)
(cid:13)
(cid:13)+O

(cid:13)
(cid:13)
(cid:13)−

−µG
t

−µt

t

t

(cid:17)

−

(cid:16) f
N

(cid:13)
(cid:13)w(i)∈G
(cid:13)

t−1 −µG

t−1

(cid:17)

(cid:13)
(cid:13)
(cid:13)+O

(cid:16) f
N

≤ O

(cid:17)

(cid:16) f
N
(36)

.

Combining (34), (35), (36), we get that for a good client, the upper bound on PID value u(i)∈G
∆max + O( f
.

(cid:16)
N ), which can be simplified to t

∆max + O( f

∆max + O( f

N ) + t

+ O( f

t
N )

N )

(cid:17)

(cid:16)

(cid:17)

is

∆max + O( f

The permissive upper bound of a threshold for the PID score of good clients {i : i ∈ G} is
(cid:16)
t
. This upper bound ensures zero false positive rate, however, the false negative
rate can be expected to be high. This bound is not usable, however, it provides a starting point for us
to derive a more tight and practical threshold.

(cid:17)
N )

1.6 Proof of Theorem 5

Proof:

(cid:104)
E[Ut] = E

∥w(i)

(cid:105)
t − µt∥

+ KI

(cid:104)
E
∥w(i)

(cid:105)
x − µx∥

t−1
(cid:88)

x=0

Using µ∆ = E[∆t]

(cid:104)
∥w(i)
+ KDE

t − µt∥ − ∥w(i)

t−1 − µt−1∥

E[Ut] = µ∆ + KI tµ∆ + 2KDµ∆ ≈ µ∆(1 + KI t)

(cid:105)

(37)

(38)

Next, derive the variance of our PID Score. Due to Bienaymé identity we would have additional
covariance terms, but those can be neglected due to assumption (1). The variance of Ut then becomes:

t = Var[Ut] = Var[∆t] + K 2
σ2
I

t−1
(cid:88)

x=0

Var[∆x] + 2KDVar[∆t] = σ2

∆ + K 2

I σ2

∆ + 2K 2

Dσ2
∆

(39)

Finally, using Chebyshev’s inequality we can state that with a probability of at least 1 − α the benign
clients will be under the threshold τ = ¯ut + zσt. Equivalently, no more than α-fraction of benign
clients exceed τ .

n
o
i
r
e
t
i
r

C

l
a
v
o
m
e
R

0.5

n
o
i
r
e
t
i
r

C

l
a
v
o
m
e
R

n
o
i
r
e
t
i
r

C

l
a
v
o
m
e
R

Round

Round

(a) KI = 0, Kd = 0

(b) KI = 0.05, Kd = 0.05

n
o
i
r
e
t
i
r

C

l
a
v
o
m
e
R

Round

Round

(c) KI = 0.6, Kd = 0.4

(d) KI = 0.8, Kd = 0.2

Figure 1: The effect of varying coefficients for the integral and derivative parts. 8 benign and 2 anomalous
datasets were generated based on the FEMNIST dataset.

1.7 Proof of Theorem 6

Proof: the Gaussian threshold follows directly from Theorem 5 under the Gaussian assumption
∆t ∼ N (µ∆, σ2
∆). If α is the desired alarm rate, than using the standard normal distribution and the
z-score corresponding to 1 − α gives us Pr[Ut > τGauss] = 1 − Φ(z1−α) = α.

2 Ablation Study: PID Coefficients

We begin our empirical study by investigating the impact of varying coefficient values of the proportional, integral, and derivative component of equation 25. First, consider Figure 1(a), where Kp = 1,
KI = 0 and Kd = 0 as well. In this case we observe a metric convergence pattern similar to the one
in Krum and Multi-Krum due to the fact that PID value degrades to the P component’s value only,
i.e. we are simply comparing the distances between each client’s submitted model weights and the
centroid. As we increase KI and Kd slightly, to 0.05, the distinction between benign and anomalous
clients becomes more prominent (Figure 1(b), anomalous clients are represented by the orange and
red curves at the top), however, the convergence of anomalous and benign clients still persists. As
we increase KI even more, the compounding effect of PID takes over, and the anomalous clients
become clearly identifiable throughout the entire learning process (Figures 1(c), 1(d)). Even though
PID acts based on the distance values, because of the compounding of the integral term we can avoid
the mix-up between the anomlaous and benign clients. For the remaining parts of our empirical
evaluation we heuristically set the values KI = 0.8 and Kd = 0.2.

3 Performance

Table 1: Last round accuracy and loss for each method across datasets. This table illustrates the results in Fig. 4
of the main paper.

Metric

Method

FEMNIST

ITS

Pneumonia

Std(FEMNIST)

Std(ITS)

Std(Pneumonia)

Accuracy

Loss

Krum
Multi-Krum
RFA
Bulyan
PID

Krum
Multi-Krum
RFA
Bulyan
PID

0.9434
0.9537
0.9730
0.9732
0.9715

0.0180
0.0107
0.0086
0.0095
0.0121

0.9057
0.9306
0.9405
0.9057
0.9306

0.1223
0.1452
0.3831
1.1076
0.2699

0.9868
0.9860
0.9895
0.9925
0.9895

0.0056
0.0112
0.0041
0.0068
0.0066

0.1719
0.1352
0.1108
0.1027
0.1311

0.0275
0.0235
0.0221
0.0222
0.0226

0.0528
0.0463
0.0452
0.0448
0.0450

0.0477
0.2551
0.1776
0.3492
0.0666

0.0094
0.0762
0.0796
0.0440
0.0092

0.0043
0.0583
0.0343
0.0234
0.0017

3.1 PID Computation Time

Total clients

Time (ms)
2.225
2.394
2.160
2.575
2.602
2.757
3.051
3.348
3.771
3.752
4.126
4.260
4.196
4.329
4.559
4.767

Table 2: This graph illustrates Fig. 4(f) of the main paper. Time required for computing the PID score as the
number of participating clients increases from 5 to 20 clients.

Code and Dataset Artifacts

The experiments were conducted on a system equipped with an AMD Ryzen 5 7600 CPU, 32 GB of
RAM, and an NVIDIA RTX 4060TI GPU with 16 GB of dedicated memory, running the Ubuntu
22.04 OS. Our code may be used for the reproduction and further reconfiguration of our experimental
setup. Additionally, it provides the ability to collect and save metrics necessary for the further
analysis. We also provide the datasets that we used to facilitate the reproduction of our empirical
study experiments. All the shared materials can be found by this anonymized link that does not
disclose the authors’ identities: https://drive.google.com/file/d/1VSTeE6ynMPQcnGUu_nIZO0_
mkQdni8DH/view?usp=drive_link.

Datasets used in the experiments are initially downloaded from AWS by the execution script and later
can be found in the datasets/ folder of the archive.

Guidelines for the experiment setup configuration and execution are included in the README with
the code artifacts found in the link above.
